{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9921c53b",
   "metadata": {},
   "source": [
    "## Graph Attention Network - Keras (Node Classification)\n",
    "\n",
    "Graph Neural Network에서는 여러 Task를 수행합니다. 대표적인 2가지의 Task는 아래와 같습니다.\n",
    "1. Node Classification: 해당 Node가 어떠한 Class에 속하는지 판별합니다.\n",
    "2. Link Prediction: Node간에 연결이 되었는지 Prediction 합니다.\n",
    "\n",
    "이번 .ipynb에서는 GNN과 GAT를 사용하였을 경우에 Node Classification의 성능이 어떠한 변화를 보이는지 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82802229",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265778bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 6)\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d3d91",
   "metadata": {},
   "source": [
    "### Obtain the dataset\n",
    "\n",
    "**Cora Dataset**\n",
    "\n",
    "2708개의 논문에 대한 내용이 담겨있는 예제입니다. 해당 Dataset의 Node는 각 논문의 ID이고, Edge는 서로 인용하였는가 입니다. 해당 Dataset에서 Label은 논문의 종류로서 총 7 가지 입니다. 기존에 사용한 Notation으로서는 다음과 같이 적을수 있습니다.\n",
    "\n",
    "- $N$: Number of nodes (2708)\n",
    "- $F$: Dimension of node (1433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda7c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = keras.utils.get_file(\n",
    "    fname=\"cora.tgz\",\n",
    "    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "    extract=True,\n",
    ")\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
    "\n",
    "citations = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.cites\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "\n",
    "papers = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.content\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"],\n",
    ")\n",
    "\n",
    "class_values = sorted(papers[\"subject\"].unique())\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6172d0",
   "metadata": {},
   "source": [
    "Citation 정보를 담고 있는 Dataset 입니다. 서로 이어진 Edge를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2a611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target  source\n",
      "0          0      21\n",
      "1          0     905\n",
      "2          0     906\n",
      "...      ...     ...\n",
      "5426    1874    2586\n",
      "5427    1876    1874\n",
      "5428    1897    2707\n",
      "\n",
      "[5429 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f544c7a",
   "metadata": {},
   "source": [
    "Paper의 정보를 가지고 있습니다. 각 Paper의 Id와 특성 그리고 Label정보를 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f4f4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      paper_id  term_0  term_1  ...  term_1431  term_1432  subject\n",
      "0          462       0       0  ...          0          0        2\n",
      "1         1911       0       0  ...          0          0        5\n",
      "2         2002       0       0  ...          0          0        4\n",
      "...        ...     ...     ...  ...        ...        ...      ...\n",
      "2705      2372       0       0  ...          0          0        1\n",
      "2706       955       0       0  ...          0          0        0\n",
      "2707       376       0       0  ...          0          0        2\n",
      "\n",
      "[2708 rows x 1435 columns]\n"
     ]
    }
   ],
   "source": [
    "print(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5316d95",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "Dataset을 Split합니다. 50% 기준으로 잘라서 Train Dataset과 Test Dataset을 구축 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56068e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain random indices\n",
    "random_indices = np.random.permutation(range(papers.shape[0]))\n",
    "\n",
    "# 50/50 split\n",
    "train_data = papers.iloc[random_indices[: len(random_indices) // 2]]\n",
    "test_data = papers.iloc[random_indices[len(random_indices) // 2 :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1787f",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Train Dataset과 Test Dataset을 Numpy -> Tensor로서 변형합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696274ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape:\t\t (5429, 2)\n",
      "Node features shape: (2708, 1433)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 20:42:18.258857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Obtain paper indices which will be used to gather node states\n",
    "# from the graph later on when training the model\n",
    "train_indices = train_data[\"paper_id\"].to_numpy()\n",
    "test_indices = test_data[\"paper_id\"].to_numpy()\n",
    "\n",
    "# Obtain ground truth labels corresponding to each paper_id\n",
    "train_labels = train_data[\"subject\"].to_numpy()\n",
    "test_labels = test_data[\"subject\"].to_numpy()\n",
    "\n",
    "# Define graph, namely an edge tensor and a node feature tensor\n",
    "edges = tf.convert_to_tensor(citations[[\"target\", \"source\"]])\n",
    "node_states = tf.convert_to_tensor(papers.sort_values(\"paper_id\").iloc[:, 1:-1])\n",
    "\n",
    "# Print shapes of the graph\n",
    "print(\"Edges shape:\\t\\t\", edges.shape)\n",
    "print(\"Node features shape:\", node_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e998d",
   "metadata": {},
   "source": [
    "### Tensorflow Function\n",
    "\n",
    "현재 Code를 보기 전에 앞서서 사용하는 Tensorflow의 함수를 이해하고 가야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e082f343",
   "metadata": {},
   "source": [
    "**Function.1 : tf.gather**\n",
    "\n",
    "인덱스의 단일 축 텐서를 전달하는 방법 이다. 현재 사용중인 Edge의 Dataset은 아래와 같이 생겼다.\n",
    "```code\n",
    "[[   0,   21],\n",
    "[   0,  905],\n",
    "[   0,  906],\n",
    "...,\n",
    "[1874, 2586],\n",
    "[1876, 1874],\n",
    "[1897, 2707]]\n",
    "```\n",
    "\n",
    "따라서, 해당 결과는 입력되는 Tensor에서 해당되는 Index 2개 (ex) [0, 21])를 concat하는 역할을 한다.\n",
    "\n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5ae480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'p2', b'p0', b'p2', b'p5'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])\n",
    "indices = [2, 0, 2, 5]\n",
    "tf.gather(params, indices).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f00b36",
   "metadata": {},
   "source": [
    "**Function.2: tf.math.unsorted_segment_sum**\n",
    "\n",
    "Segmentation을 진하는 연산이다. 아래 코드를 보면 이해가 빠르다. 같은 Segment끼리 합해주는 과정을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc70ef",
   "metadata": {},
   "source": [
    "Case 1: c의 0, 2번째가 같은 Cluster이고, 1번째가 다른 Cluster 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da3a3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 5, 5, 5],\n",
       "       [5, 6, 7, 8]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]\n",
    "tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5457d12",
   "metadata": {},
   "source": [
    "Case 2: c의 0, 1번째가 같은 Cluster이고, 2번째가 다른 Cluster 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25c0a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  8, 10, 12],\n",
       "       [ 4,  3,  2,  1]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]\n",
    "tf.math.unsorted_segment_sum(c, [0, 0, 1], num_segments=2).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5d9f1",
   "metadata": {},
   "source": [
    "Case 3: c의 0, 1번째가 같은 Cluster이고, 2번째가 다른 Cluster 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010aa409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  3,  2,  1],\n",
       "       [ 6,  8, 10, 12]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]\n",
    "tf.math.unsorted_segment_sum(c, [1, 1, 0], num_segments=2).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb81e1a",
   "metadata": {},
   "source": [
    "위의 Case를 살표보면, <code>tf.math.unsorted_segment_sum</code>은 같은 Cluster값을 더해주며, 순서는 Cluter의 Label 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d79d4",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**사용시 주의 사항**</span>\n",
    "\n",
    "위의 Code중에서 <code>tf.repeat(attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\")))</code>부분 때문에 Edge의 값은 항상 Sorting되어 있어나 같은 값들이 뭉쳐있어야 한다. 즉, Edge가 무조건 Sorting되어 있다고 가정하고 사용하는 Code 이다. (Graph쪽은 Dataset을 항상 이렇게 구축하는지는 잘 모르겠습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db30ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 1 4 4 7 7 7], shape=(8,), dtype=int32)\n",
      "tf.Tensor([1 1 1 4 4 7 7 7], shape=(8,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a_s = [[1],\n",
    "       [4],\n",
    "       [7]]\n",
    "\n",
    "e = [0, 0, 0, 1, 1, 2, 2, 2]\n",
    "\n",
    "a_s_t = tf.constant(a_s)\n",
    "e_t = tf.constant(e)\n",
    "print(tf.repeat(a_s_t, tf.math.bincount(e_t)))\n",
    "\n",
    "a_s = [[1],\n",
    "       [4],\n",
    "       [7]]\n",
    "\n",
    "e = [0, 0, 1, 0, 1, 2, 2, 2]\n",
    "\n",
    "a_s_t = tf.constant(a_s)\n",
    "e_t = tf.constant(e)\n",
    "print(tf.repeat(a_s_t, tf.math.bincount(e_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96227c",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n",
    "\n",
    "![png](./tmp/img/2.png)\n",
    "\n",
    "아래 Code는 Graph Attention Network에서 Attention Mechanism에 대한 Layer입니다. 수식을 대입해서 확인하기 정의해야하는 Notation과 해당 부분은 아래와 같습니다. 또한, Dimension이 헛갈리기 때문에 예시로 실제 값을 적어두겠습니다.\n",
    "\n",
    "**Notation**\n",
    "- <code>input_shape[0][-1]</code>: Dimension of Feature ($F$) (1433)\n",
    "- <code>self.units</code>: Dimension of hidden Layer ($F^{'}$) (4)\n",
    "- <code>self.kernel</code>: Weight (Trainable Parameter) ($W \\in \\mathbb{R}^{F \\times F^{'}}$) (1433 x 4)\n",
    "- <code>self.kernel_attention</code>: For Attention( Trainable Parameter) ($a \\in \\mathbb{R}^{2F^{'}}$) (8 x 1)\n",
    "\n",
    "해당 Model에서 Input으로 들어오는 정보는 아래와 같습니다.\n",
    "\n",
    "**Inputs**\n",
    "- <code>node_states</code>: Input ($\\vec{h} = \\{\\vec{h_1}, \\vec{h_2}, \\ldots, \\vec{h_N} \\} \\in \\mathbb{R}^{N \\times F}$) (2708 x 4)\n",
    "- <code>edges</code>: For Attention (Node간에 이여져 있는지 확인하기 위하여 사용) (5429, 2)\n",
    "    - 0: i-th Node\n",
    "    - 1: j-th Node\n",
    "    \n",
    "아래 구현된 Code와 논문에서 제시한 방법을 대입하면 아래와 같습니다.\n",
    "\n",
    "**Attention Mechanism**\n",
    "- #1: $W \\vec{h}$ // Input을 Weight를 곱하여 Latent Representation으로 보낸다.\n",
    "    - <code>node_states_transformed = tf.matmul(node_states, self.kernel)</code> (2708 x 4)\n",
    "\n",
    "- #2: $(W\\vec{h_i}, W\\vec{h_j})$ // Concat을 한다. 여기서 뒤따라 오는 수식 때문에, 모든 조합에 대하여 계산할 필요는 없다. 연결된 조합에 대하여서만 Concat 필요하며, 이는 edges에 정보가 담겨있다. Output은 조합된 개수(Edge shape[0] x attemtion dimension)으로서 나온다.\n",
    "    - <code>tf.gather(node_states_transformed, edges)</code> (5429, 2, 4)\n",
    "    - <code>node_states_expanded = tf.reshape(node_states_expanded, (tf.shape(edges)[0], -1))</code> (5429, 8)\n",
    "    \n",
    "- #3: $LeakyReLU(a(W\\vec{h_i}, W\\vec{h_j}))$ // Attention Score를 구한다.\n",
    "    - <code>attention_scores = tf.nn.leaky_relu(tf.matmul(node_states_expanded, self.kernel_attention))</code> (5429, 1)\n",
    "    - <code>attention_scores = tf.squeeze(attention_scores, -1)</code> (5429,)\n",
    "    \n",
    "- #4: $exp(LeakyReLU(e_{ik}))$ // Exponential을 취한다. (Softmax 계산을 위해)\n",
    "    - <code>attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))</code> (5429, )\n",
    "- #5: $\\sum_{k \\in N_i} exp(LeakyReLU(e_{ik}))$ // 연결된 조합에 대한 Attention Score의 합을 구한다. 해당 과정에 대해서는 아래 2 Step으로서 이루워진다.\n",
    "    - <code>attention_scores_sum = tf.math.unsorted_segment_sum(data=attention_scores, segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,)</code>: (edges[:, 0]의 개수-1898, ) // 나를 기준으로 이어진 Node는 같은 Segments로서 지정하여 Summation 취한다. 따라서 해당 Output은 기준이 되는 Edge의 Unique한 개수가 된다.\n",
    "    - <code>attention_scores_sum = tf.repeat(attention_scores_sum, \n",
    "    tf.math.bincount(tf.cast(edges[:, 0], \"int32\")))</code>: (5429,) // Repeat하여 개수를 증가시킨다. Attention Distribution을 계산하기 위해서 이다.\n",
    "\n",
    "- #6: $\\alpha_{ij} = \\frac{exp( LeakyReLU(e_{ij}))}{\\sum_{k \\in N_i} exp(LeakyReLU(e_{ik}))}$ // Attention Distribution을 구한다.\n",
    "    - <code>attention_scores_norm = attention_scores / attention_scores_sum</code>\n",
    "\n",
    "- #7: $\\vec{h^{'}_i} = \\sigma(\\sum_{j \\in N_j} \\alpha_{ij} W \\vec{h_j})$ // Graph Attention 값을 구한다. Output은 해당 노드와 주변 노드간의 Weight및 Attention을 곱한 값으로서 표현된다. \n",
    "    - <code>node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])</code> (5429, 4) // Output으로 뱉고자 하는 Node ($\\vec{h_i}^{'}$)와 연결되 Node의 특성 ($W\\vec{h_j}$)을 가져온다.\n",
    "    - <code>out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0],)</code>  (2708, 4) // Output ($\\vec{h_i}^{'}$)은 이어져 있는 주변 Node의 정보 ($W\\vec{h_j}$)와 Attention Score(해당 Node i에 대하여 주변에 연결되어 있는 Node들 중에 특정 Node j가 영향을 미치는 정도)의 곱으로서 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b8f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # Linearly transform node states\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        # (1) Compute pair-wise attention scores\n",
    "        node_states_expanded = tf.gather(node_states_transformed, edges)\n",
    "        node_states_expanded = tf.reshape(\n",
    "            node_states_expanded, (tf.shape(edges)[0], -1)\n",
    "        )\n",
    "        attention_scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(node_states_expanded, self.kernel_attention)\n",
    "        )\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "\n",
    "        # (2) Normalize attention scores\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(\n",
    "            attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\"))\n",
    "        )\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        # (3) Gather node states of neighbors, apply attention scores and aggregate\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0],\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102580d",
   "metadata": {},
   "source": [
    "### (Multi-head) graph attention layer\n",
    "\n",
    "해당 부분은 간단한 부분입니다.\n",
    "\n",
    "![png](./tmp/img/3.png)\n",
    "\n",
    "위의 Figure와 같이 여러 Attention Layer에서 나온 Output을 Concat후 Average하여 사용합니다.\n",
    "$$\\vec{h^{'}_i} = \\sigma(\\frac{1}{K} \\sum_{k=1}^K \\sum_{j \\in N_j} \\alpha_{ij}^k W^k \\vec{h_j})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "\n",
    "        # Obtain outputs from each attention head\n",
    "        outputs = [\n",
    "            attention_layer([atom_features, pair_indices])\n",
    "            for attention_layer in self.attention_layers\n",
    "        ]\n",
    "        # Concatenate or average the node states from each head\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        # Activate and return node states\n",
    "        return tf.nn.relu(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc0d5c",
   "metadata": {},
   "source": [
    "### Graph Attention Network\n",
    "\n",
    "해당 모델의의 Architecture는 총 3개의 Layer로서 구성되어있습니다.\n",
    "- 1st Layer: <code>self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")</code> // 간단한 Dense Layer로서 처음에 많은 Feature를 적은수의 Feature로서 변환한다.\n",
    "- 2nd Layer: <code>self.attention_layers = [MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)]</code>: num_layers만큼 MultiHead Graph Attention Network를 적용한다. 몇개 self attention을 사용하는 지는 num_heads로서 지정한다.\n",
    "- 3rd Layer: <code>self.output_layer = layers.Dense(output_dim)</code>: 간단하게 Dense Layer로서 Node Classification을 실시한다.\n",
    "\n",
    "해당 Model Architecture에서 주의하여서 봐야하는 점은 아래와 같다.\n",
    "\n",
    "**Appendix #1. No Edge Information**\n",
    "```python\n",
    "for attention_layer in self.attention_layers:\n",
    "    x = attention_layer([x, edges]) + x\n",
    "```\n",
    "\n",
    "위의 Code를 보게 되면, Input에서 받게 되는 정보의 Edge Dataset은 자기 자신은 이어져 있다고 (Self-Connection)생각하지 않습니다. 하지만, 이러한 자신의 정보를 사용하지 않고 주변 정보만을 사용하게 된다면, Performance는 매우 낮은 것을 확인할 수 있습니다. (대략 30% 감소) 따라서 위와 같이 자기 자신의 정보를 그대로 사용하는 과정이 필요합니다.\n",
    "\n",
    "해당 과정을 수식으로 쓰면 다음과 같습니다.\n",
    "$$\\vec{h^{'}_i} = \\sigma(\\frac{1}{K} \\sum_{k=1}^K \\sum_{j \\in N_j} \\alpha_{ij}^k W^k \\vec{h_j})$$\n",
    "$$\\rightarrow \\vec{h^{'}_i} = \\sigma(\\frac{1}{K} \\sum_{k=1}^K \\sum_{j \\in N_j, j \\neq i} \\alpha_{ij}^k W^k \\vec{h_j}) + \\vec{h_i}$$\n",
    "\n",
    "위의 수식을 사용하게 되면, 자기 자신의 정보를 중요하게 생각하면서, 다른 주변 연결된 Node 중에서 중요한 것을 강조하여 사용하는 형태의 수식이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7edd17",
   "metadata": {},
   "source": [
    "**Appendix #2. Indexing**\n",
    "```python\n",
    "def train_step(self, data):\n",
    "    indices, labels = data\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        \n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, self.trainable_weights)\n",
    "    # Apply gradients (update weights)\n",
    "    optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    # Update metric(s)\n",
    "    self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "```\n",
    "\n",
    "현재 Dataset을 구축할 때, Node를 기준으로 나누었다. 또한, Node에 대한 연결정보를 사용하기 위하여 아래와 같은 Dataset으로서 Graph를 구성하였다.\n",
    "\n",
    "- <code>node_states</code>: (2708, 1433)\n",
    "- <code>edges</code>: (5429, 2)\n",
    "\n",
    "구축한 Dataset에서 GAT의 Output은 (2708, num_classes)로서 모든 Node에 대한 정보가 들어가있다. 때문에 위와 같이 <code>tf.gather(outputs, indices)</code>로서 train sample에 대한 probability (1304, num_classes)와 train label을 비교하면서 학습을 진행하여야 한다.\n",
    "\n",
    "위에서 train_step 안의 코드는 형식화 되있는 부분이며, 이해가 되지 않으면 <a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit?hl=ko\">Tensorflow Model-fit</a>를 참조하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e6d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_states,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_states = node_states\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        x = self.preprocess(node_states)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) + x\n",
    "            \n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            outputs = self([self.node_states, self.edges])\n",
    "            # Compute loss\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute probabilities\n",
    "        return tf.nn.softmax(tf.gather(outputs, indices))\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64488cc7",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f75e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 55s - loss: 1.9140 - acc: 0.2718 - val_loss: 1.7719 - val_acc: 0.4265 - 55s/epoch - 11s/step\n",
      "Epoch 2/100\n",
      "5/5 - 11s - loss: 1.4025 - acc: 0.5181 - val_loss: 1.1628 - val_acc: 0.6618 - 11s/epoch - 2s/step\n",
      "Epoch 3/100\n",
      "5/5 - 9s - loss: 0.8099 - acc: 0.7668 - val_loss: 0.8047 - val_acc: 0.8088 - 9s/epoch - 2s/step\n",
      "Epoch 4/100\n",
      "5/5 - 6s - loss: 0.4824 - acc: 0.8645 - val_loss: 0.8099 - val_acc: 0.7794 - 6s/epoch - 1s/step\n",
      "Epoch 5/100\n",
      "5/5 - 6s - loss: 0.2931 - acc: 0.9113 - val_loss: 0.8186 - val_acc: 0.8235 - 6s/epoch - 1s/step\n",
      "Epoch 6/100\n",
      "5/5 - 11s - loss: 0.1972 - acc: 0.9483 - val_loss: 0.7035 - val_acc: 0.8088 - 11s/epoch - 2s/step\n",
      "Epoch 7/100\n",
      "5/5 - 11s - loss: 0.1096 - acc: 0.9819 - val_loss: 0.7641 - val_acc: 0.8088 - 11s/epoch - 2s/step\n",
      "Epoch 8/100\n",
      "5/5 - 10s - loss: 0.0686 - acc: 0.9918 - val_loss: 0.7421 - val_acc: 0.8162 - 10s/epoch - 2s/step\n",
      "Epoch 9/100\n",
      "5/5 - 9s - loss: 0.0480 - acc: 0.9951 - val_loss: 0.7110 - val_acc: 0.8529 - 9s/epoch - 2s/step\n",
      "Epoch 10/100\n",
      "5/5 - 7s - loss: 0.0326 - acc: 0.9975 - val_loss: 0.7396 - val_acc: 0.8309 - 7s/epoch - 1s/step\n",
      "Epoch 11/100\n",
      "5/5 - 5s - loss: 0.0240 - acc: 0.9984 - val_loss: 0.7814 - val_acc: 0.8309 - 5s/epoch - 933ms/step\n",
      "Epoch 12/100\n",
      "5/5 - 5s - loss: 0.0191 - acc: 0.9984 - val_loss: 0.8141 - val_acc: 0.8382 - 5s/epoch - 1s/step\n",
      "Epoch 13/100\n",
      "5/5 - 4s - loss: 0.0153 - acc: 0.9992 - val_loss: 0.8394 - val_acc: 0.8309 - 4s/epoch - 863ms/step\n",
      "Epoch 14/100\n",
      "5/5 - 6s - loss: 0.0124 - acc: 0.9992 - val_loss: 0.8593 - val_acc: 0.8309 - 6s/epoch - 1s/step\n",
      "----------------------------------------------------------------------------\n",
      "Test Accuracy 79.3%\n"
     ]
    }
   ],
   "source": [
    "# Define hyper-parameters\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = len(class_values)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 3e-1\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.SGD(LEARNING_RATE, momentum=MOMENTUM)\n",
    "accuracy_fn = keras.metrics.SparseCategoricalAccuracy(name=\"acc\")\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\", min_delta=1e-5, patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Build model\n",
    "gat_model = GraphAttentionNetwork(\n",
    "    node_states, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[accuracy_fn])\n",
    "\n",
    "gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "_, test_accuracy = gat_model.evaluate(x=test_indices, y=test_labels, verbose=0)\n",
    "\n",
    "print(\"--\" * 38 + f\"\\nTest Accuracy {test_accuracy*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
