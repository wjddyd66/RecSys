{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Graph Collaborative Filtering\n",
    "- Paper Link: https://arxiv.org/pdf/1905.08108v2.pdf\n",
    "- PyTorch Code: https://github.com/huangtinglin/NGCF-PyTorch\n",
    "- PyTorch Code2: https://bladejun.tistory.com/67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    ">Learning vector representations (aka. embeddings) of users and\n",
    "items lies at the core of modern recommender systems. Ranging\n",
    "from early matrix factorization to recently emerged deep learning\n",
    "based methods, existing efforts typically obtain a user’s (or an\n",
    "item’s) embedding by mapping from pre-existing features that\n",
    "describe the user (or the item), such as ID and attributes. **We\n",
    "argue that an inherent drawback of such methods is that, the\n",
    "collaborative signal, which is latent in user-item interactions,\n",
    "is not encoded in the embedding process.** As such, the resultant\n",
    "embeddings may not be sufficient to capture the collaborative\n",
    "filtering effect.  \n",
    "In this work, **we propose to integrate the user-item interactions —\n",
    "more specifically the bipartite graph structure — into the embedding\n",
    "process. We develop a new recommendation framework Neural\n",
    "Graph Collaborative Filtering (NGCF), which exploits the useritem graph structure by propagating embeddings on it.** This leads\n",
    "to the expressive modeling of high-order connectivity in useritem graph, effectively injecting the collaborative signal into the\n",
    "embedding process in an explicit manner. We conduct extensive\n",
    "experiments on three public benchmarks, demonstrating significant\n",
    "improvements over several state-of-the-art models like HOPRec [40] and Collaborative Memory Network [5]. Further analysis\n",
    "verifies the importance of embedding propagation for learning\n",
    "better user and item representations, justifying the rationality and\n",
    "effectiveness of NGCF. Codes are available at https://github.com/\n",
    "xiangwang1223/neural_graph_collaborative_filtering.\n",
    "\n",
    "기존의 GNN 기반의 Model들을 살펴보게 되면, 주변 Node들의 관계를 고려하여 Prediction이 가능한 Model들이 제시되었다. 하지만, 이러한 Model들의 단점은 Entitiy가 단 하나였다는 것 이다. 즉, User가 어떠한 Class이냐 혹은 Item이 어떠한 Class인지에 대해서만 Prediction을 진행하였다는 것 이다.\n",
    "\n",
    "현재, NGCF는 이러한 모델들의 문저점은 **Interaction**을 고려하지 못했다는 것 이다. 해당 논문 저자는 이러한 문제점을 해결하기 위하여, **User-Item Relation**을 고려할 수 있는 Collaborative Filtering을 기존 GNN에 추가한 Model을 제안한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    ">Collaborative filtering (CF) addresses it by assuming that behaviorally similar users would exhibit similar preference\n",
    "on items. To implement the assumption, a common paradigm is to parameterize users and items for reconstructing historical interactions, and predict user preference based on the parameters [1,\n",
    "14].  \n",
    "two key components in learnable CF models — 1) embedding, which transforms users and items to vectorized representations, and 2) interaction modeling, which reconstructs historical interactions based on the embeddings.  \n",
    "Despite their effectiveness, we argue that these methods are not sufficient to yield satisfactory embeddings for CF. The key reason is that the embedding function lacks an explicit encoding of the crucial\n",
    "collaborative signal, which is latent in user-item interactions to reveal the behavioral similarity between users (or items).  \n",
    "In this work, we tackle the challenge by exploiting the high-order connectivity from user-item interactions, a natural way that encodes collaborative signal in the interaction graph structure.\n",
    "\n",
    "해당 논문의 Introduction에서 주요하다고 생각하는 4 문장이다. 현재, 기존 문제점은 다음과 같다.\n",
    "1. GNN 기반의 Model은 User-Item간의 Interaction을 고려하지 못한다.\n",
    "2. CF 기반의 Model은 User-Item간의 Interaction을 고려하지만, GNN이 고려할 수 있는 User-User Relation을 고려하지 못한다.\n",
    "3. 이러한 문제점을 해결하기 위해서는 각각의 문제점을 해결하여야 한다.\n",
    "    - (1) CF기반의 Model을 만들기 위해서는 User, Item을 각각 Embedding을 할 수 있어야 한다. 또한, 이 두 Vector를 활용하여, User-Item간의 Intercation을 나타낼 수 있어야 한다.\n",
    "    - (2) User-User Intercation과 같은 GNN의 기반의 Model로서 학습하기 위해서는, High-Order Connectivity을 고려할 수 있어야 한다. 즉, Hop을 키우면 키울수록 고려해야 하는 Dataset의 크기는 매우 커지게 된다. 이러한 문제를 해결할 수 있어야 한다. -> **Embedding Propagation으로서 해결 한다.**\n",
    "    \n",
    "먼저, High-Order Interaction을 고려해야 하는 이유를 살펴보자.\n",
    "![png](../img/NGCF/img1.png)\n",
    "\n",
    "위의 그림을 살펴보게 되면, 먼저 왼쪽 Sub Figure을 User와 Item간의 Interaction을 나타내는 Figure이다. 오른쪽 Sub Figure는 Hop(<span>$l$</span>)을 늘려가면서 관계를 살펴보는 단계이다. 해당 그림에서는 크게 2가지를 알 수 있다.\n",
    "\n",
    "첫째, User간의 Interaction을 알 수 있다. 예를 들어 Hop=1(<span>$l=1$</span>)인 경우에는 User-User Interaction을 고려할 수 없다. 하지만, Hop=2(<span>$l=2$</span>)인 경우에는 <span>$u_2 \\rightarrow i_2 \\rightarrow u_1$</span>으로서 <span>$<u_1, u_2>$</span>의 관계가 있다는 것을 알 수 있다. 즉, 같은 Item(<span>$i_2$</span>)을 선호하는 있다는 관계를 고려할 수 있다는 것 이다.\n",
    "\n",
    "둘째, 알 수 없던 관계를 고려할 수 있다. 예를 들어 Hop=1(<span>$l=1$</span>)인 경우에는 <span>$u_1$</span>과 <span>$l_4$</span>는 전혀 관계가 없다. 하지만 Hop=3(<span>$l=3$</span>)인 경우에는 <span>$<u_1, i_4>$</span>의 관계가 무려 2개로서 제일 많게 변하는 것을 알 수 있다. 이러한 High-Order Connectivity를 고려하면 할 수록, 이전에는 발견할 수 없는 관계를 고려할 수 있으며, 주요한 Interaction을 찾을 수 있다는 것 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "![png](../img/NGCF/img2.png)\n",
    "\n",
    "위의 Figure는 <span>$u_1$</span>과 <span>$i_4$</span>간의 Interaction을 Prediction하는 Figure이다. 해당 그림을 살펴보게 되면, 아래와 같이 3단계로 이루워진다.\n",
    "- (1) User와 Item을 각각 Embedding한다.\n",
    "- (2) User와 Item을 각각 Embedding Propagation을 진행한다. 해당 과정에서 Interaction을 고려하게 된다.\n",
    "- (3) 주변 User와 Item을 고려하여 Interaction을 Prediction한다.\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "먼저 Embedding Lookup Table (<span>$E$</span>)는 아래와 같이 선언된다.\n",
    "<p>$$E=[\\underbrace{e_{u_1}, \\ldots, e_{u_N}}_{\\text{User Embeddings}}, \\underbrace{e_{i_1}, \\ldots, e_{i_N}}_{\\text{Item Embeddings}}].$$</p>\n",
    "\n",
    "- <span>$e_u \\in \\mathbb{R}^{d}$</span>\n",
    "- <span>$e_i \\in \\mathbb{R}^{d}$</span>\n",
    "\n",
    "해당 과정에서 주요한 점은 같은 Diemnsion의 크기로서 Embedding을 진행한다는 것 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Propagation Layers\n",
    "먼저, GNN의 message-passing architecture에서 CF signal(User-Item Interaction)을 고려하기 위하여 어떻게 Architecture를 구성하였는지 알아보자. 아래 수식은 one-layer propagation에 대하여 먼저 알아보고, multiple successive layers에 대하여 알아보자.\n",
    "\n",
    "#### First-order Propagation\n",
    "먼저, user's preference와 item간의 direct한 관계를 고려하기 위한 embedding propagation에 대해 알아보자. 해당 과정은 message construction과 message aggregation으로서 이루워진다.\n",
    "\n",
    "**Message Construction**  \n",
    "먼저, Adjancy Matrix에서 이어진 <span>$(u, i)$</span>에 대하여 i가 u에 전달하는 message를 아래와 같이 정의하였다.\n",
    "<p>$$m_{u \\leftarrow i} = f(e_i, e_u, p_{ui})$$</p>\n",
    "\n",
    "- <span>$m_{u \\leftarrow i}$</span>: Message Embedding\n",
    "- <span>$f(\\cdot)$</span>: Message encoding function\n",
    "- <span>$p_{ui}$</span>: Edge(u,i)의 propagation에 대한 decay factor를 조절하기 위한 인자 이다.\n",
    "\n",
    "Embeding Function (<span>$f(\\cdot)$</span>)은 아래와 같이 정의하였다.\n",
    "<p>$$m_{u \\leftarrow i} = \\frac{1}{\\sqrt{\\| N_u \\| \\| N_i\\|}}(W_1 e_i + W_2(e_i \\odot e_u))$$</p>\n",
    "\n",
    "- <span>$W_1, W_2 \\in \\mathbb{R}^{d' \\times d}$</span>: Trainable Weight\n",
    "- <span>$\\odot$</span>: Element-wise product\n",
    "- <span>$d'$</span>: Transformation Size\n",
    "- <span>$p_{ui} = \\frac{1}{\\sqrt{\\| N_u \\| \\| N_i\\|}}$</span>: Laplacian Norm\n",
    "- <span>$N_i, N_u$</span>: First-hop neighbors of user and item\n",
    "\n",
    "\n",
    "위의 수식을 살펴보게 되면, 주요하게 살펴보아야 할 점은 두 가지 이다.  \n",
    "**첫째, <span>$W_2(e_i \\odot e_u)$</span>의 수식을 통하여 User와 Item간의 Interaction을 고려할 수 있다.**  \n",
    "**둘째, Laplacian Norm으로서 Normalization을 실시해야 했다는 것 이다.**\n",
    "\n",
    "Laplacian Norm에 대해 생각하면, 아래 그림과 같다.\n",
    "\n",
    "![png](https://wikidocs.net/images/page/176711/hop.png)<br>\n",
    "그림출처: <a href=\"https://wikidocs.net/176711\">wikidocs</a>\n",
    "\n",
    "위의 그림을 살펴보게 되면, <span>$p_{ui}$</span>의 의미를 알 수 있다.  \n",
    "**<span>$p_{ui}$</span>는 아이템 i가 유저 u의 선호도에 얼만큼 공헌했는지를 의미한다. 아이템 i를 소비한 유저가 많다면,  그 중 한 유저에게 보네는 영향력이 작아지도록 Normalization을 실시한다는 의미이다. 또한, Message Passing관점에서 살펴보게 되면, Hop이 길어짐에 따라 점차적으로 공헌도가 감소한다는 특징이 있다.**\n",
    "\n",
    "**Message Aggregation**  \n",
    "Item과 User간의 Interaction을 나타낼 수 있는 One Layer Propagation을 나타내었다면, 이제 Aggregation을 어떻게 실시하는지 알아보자. Message Aggregation은 아래와 같이 나타낸 다.\n",
    "\n",
    "<p>$$e_{u}^{(1)} = \\text{LeakyReLU}(m_{u \\leftarrow u} + \\sum_{i \\in N_u}m_{u \\leftarrow i})$$</p>\n",
    "\n",
    "위의 수식을 살펴보게 되면, Message Construction에서 살펴본, <span>$m_{u \\leftarrow i}$</span>의 합과 <span>$m_{u \\leftarrow u}$</span>가 추가적으로 사용되는 것을 알 수 있다. \n",
    "\n",
    "<p>$$m_{u \\leftarrow u} = W_1 e_u$$</p>\n",
    "\n",
    "- <span>$W_1$</span>: The weight matrix shared with the one used in message construction\n",
    "\n",
    "위의 수식에서는 - <span>$m_{u \\leftarrow u}$</span>는 Self-Connection으로서 GCN에서 Residual Network와 같은 역할을 한다고 할 수 있다.\n",
    "\n",
    "**최종적으로 First-Order Propagation을 살펴보면 아래와 같이 정리할 수 있다.**\n",
    "1. 해당 논문에서 CF Signal (User-Item Interaction)을 잡기 위하여 <span>$W_2(e_i \\odot e_u)$</span>을 사용하였다.\n",
    "2. Normalization을 실시하기 위하여 <span>$p_{ui}$</span>를 사용하였다. 해당 Normalization을 통하여 User-User Relationship을 이어주는 Item의 경우, 많은 사람이 사용할 수록 Contirubtion이 적도록 Normalization을 실시하였다.\n",
    "3. <span>$m_{u \\leftarrow u}$</span>을 통하여 기존의 자기 자신의 정보를 그대로 전파하는 Self-Connection의 구조를 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-order Propagation\n",
    "![png](../img/NGCF/img3.png)\n",
    "\n",
    "1차원 Layer가 아닌 여러 Layer를 쌓은 High-Order Propagation의 형태로 일반화 한 수식은 아래와 같다.\n",
    "\n",
    "<p>$$e_{u}^{(l)} = \\text{LeakyReLU}(m_{u \\leftarrow u}^{(l)} + \\sum_{i \\in N_u}m_{u \\leftarrow i}^{(l)})$$</p>\n",
    "\n",
    "- <span>$m_{u \\leftarrow i}^{(l)} = \\frac{1}{\\sqrt{\\| N_u \\| \\| N_i\\|}}(W_1^{(l)} e_i^{(l-1)} + W_2^{(l)}(e_i^{(l-1)} \\odot e_u^{(l-1)}))$</span>\n",
    "- <span>$m_{u \\leftarrow u} = W_1^{(l)} e_u^{(l-1)}$</span>\n",
    "- <span>$W_1^{(l)}, W_2^{(l)} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$</span>: Trainable Transform\n",
    "\n",
    "**Propagation Rule in Matrix Form**  \n",
    "위의 수식은 하나의 Element에 관한 수식이다. 해당 수식을 Matrix에 적용하여 계산하기 위해서는 추가적으로 수식을 변경해야 한다. 우리가 Coding을 할 최종적인 수식은 아래와 같다.\n",
    "\n",
    "\n",
    "<p>$$E^{(l)} = \\text{LeakyReLU}((L+I)E^{(l-1)}W_1^{(l)} + LE^{(l-1)} \\odot E^{(l-1)}W_2^{(l)})$$</p>\n",
    "\n",
    "- <span>$E^{(l)} \\in \\mathbb{R}^{(N+M) \\times d_l}$</span>: The Representations for users and items.\n",
    "- <span>$L = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$</span>: Laplacian Matrix\n",
    "    - <span>$A = \\begin{bmatrix}\n",
    "                   0 & R \\\\\n",
    "                   R^T & 0\n",
    "                    \\end{bmatrix}$</span>\n",
    "    - <span>$L_{ui} = 1/\\sqrt{\\| N_u \\| \\| N_i \\|}$</span>\n",
    "- <span>$R \\in \\mathbb{R}^{N \\times M}$</span>\n",
    "    - <span>$N$</span>: Number of users\n",
    "    - <span>$M$</span>: Number of movies\n",
    "\n",
    "Matrix Propagation으로서 나타내어 매우 복잡해 보이지만, 간단한 수식 입니다. 해당 수식에 대해서는 <a href=\"https://wikidocs.net/176711\">wikidocs</a>에서 매우 자세히 설명되어 있습니다.\n",
    "\n",
    "해당 Post에 아래 부분은 해당 <a href=\"https://wikidocs.net/176711\">wikidocs</a>의 내용을 가져왔습니다.\n",
    "\n",
    "Propagation Rule in Matrix Form는 아래와 같이 <span>$L$</span> Matrix에 대해서 이해하면 수식이 쉬워집니다. 해당 Matrix를 구하기 위해서는 크게 아래와 같이 2가지 Step으로서 구할 수 있습니다.\n",
    "\n",
    "**Step 1. <span>$A, D$</span>를 구한다.**  \n",
    "![png](https://wikidocs.net/images/page/176711/laplacian1.png)<br>\n",
    "그림출처: <a href=\"https://wikidocs.net/176711\">wikidocs</a>\n",
    "\n",
    "**Step 2. <span>$L = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$</span>를 구한다.**  \n",
    "![png](https://wikidocs.net/images/page/176711/laplacian2.png)<br>\n",
    "![png](https://wikidocs.net/images/page/176711/laplacian3.png)<br>\n",
    "그림출처: <a href=\"https://wikidocs.net/176711\">wikidocs</a>\n",
    "\n",
    "위의 수식의 의미를 생각하면 <span>$L_{ui} = 1/\\sqrt{\\| N_u \\| \\| N_i \\|}$</span>을 만족하는 것을 알 수 있다.\n",
    "\n",
    "위의 <span>$L$</span>를 활용하면 <span>$(L+I)E^{(l-1)}$</span>의 수식의 의미는 아래와 같이 이해할 수 있다.\n",
    "\n",
    "![png](https://wikidocs.net/images/page/176711/laplacian4.png)<br>\n",
    "그림출처: <a href=\"https://wikidocs.net/176711\">wikidocs</a>\n",
    "\n",
    "즉, 최종적인 아래 수식은 Term 2개로서 각각 이해할 수 있다.  \n",
    "<p>$$E^{(l)} = \\text{LeakyReLU}(\\underbrace{ (L+I)E^{(l-1)} }_{\\text{Term 1}} W_1^{(l)} + \\underbrace{ LE^{(l-1)} \\odot E^{(l-1)} }_{\\text{Term 2}} W_2^{(l)})$$</p>\n",
    "\n",
    "- Term 1: 자기 자신의 정보 뿐만 아니라 이어져있는 Item Node간의 관계를 고려한 값\n",
    "- Term 2: User와 Item Interaction을 고려한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction\n",
    "\n",
    "Model Prediction은 아래와 같이 정의된다.\n",
    "<p>$$\\hat{y}_{\\text{NGCF}}(u,i) = e_u^{*T} e_i^*$$</p>\n",
    "\n",
    "- <span>$e_u^* = e_u^{(0)} \\| \\ldots \\|e_u^{(l)}$</span>\n",
    "- <span>$e_i^* = e_i^{(0)} \\| \\ldots \\|e_i^{(l)}$</span>\n",
    "- <span>$l$</span>: Number of layers\n",
    "- <span>$\\| $</span>: Concat\n",
    "\n",
    "**해당 논문에서 Prediction은 위와 같이 정의하였고, 중요한 점은 모든 Layer를 Concat하여 Prediction에 사용하였다는 것 이다. 이러한 이유에 대하여 해당 논문에서는 아래와 같이 설명하고 있다.**\n",
    "\n",
    ">Since the representations obtained in different layers emphasize the messages passed over different connections, they have different contributions in reflecting user preference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "<p>$$\\text{Loss} = \\sum_{u,i,j \\in O} - \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj}) + \\lambda \\| \\Theta \\|_2^2$$</p>\n",
    "\n",
    "- <span>$O = \\{ (u,i,j)|(u,i) \\in R^+, (u,j) \\in R^- \\}$</span>\n",
    "- <span>$R^+$</span>: Observed Interastions\n",
    "- <span>$R^-$</span>: Unobserved Interactions\n",
    "- <span>$\\Theta = \\{ E, \\{ W_1^{(l)}, W_2^{(l)} \\}_{l=1}^L \\}$</span>: Trainable model parameters\n",
    "\n",
    "\n",
    "해당 Loss는 BRP Loss라고 불리는 형태 이다. 해당 Loss의 의미는 사용한 Item과 사용하지 않은 Item을 고려할 수 있으며, 특히 사용한 Item을 더 고려한다는 Loss Format이라고 알려져 있다. 자세한 내용은 <a href=\"https://killerwhale0917.tistory.com/27\">killerwhale0917 Blog</a>를 참조하다. (아직, 해당 Loss에 대해서는 자세히 공부하지 않았습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "해당 논문에서는 top-K recommendation and preference ranking으로서 performance를 비교하였다. recall@K, ndcg@K로서 Performance를 측정하였으며, K=20으로 설정하였다.\n",
    "\n",
    "#### Dataset\n",
    "해당 논문에서 사용한 Dataset은 3개이며, 각각은 아래와 같다.\n",
    "\n",
    "![png](../img/NGCF/img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
